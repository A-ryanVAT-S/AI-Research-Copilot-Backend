# ğŸ¤– AI Research Copilot - Backend

A **FastAPI-based intelligent backend** that processes research papers with **instant AI capabilities**. Upload a PDF and get immediate access to summarization, translation, and Q&A functionality.

## ğŸŒŸ Key Features

### ğŸ“„ **Instant Processing**
- **Immediate summarization** upon PDF upload
- **Pre-built Q&A index** for instant question answering
- **Zero waiting time** for core functionality

### ğŸ¤– **AI-Powered Capabilities**
- **Intelligent Summarization**: BART-Large CNN model for research papers
- **Multi-language Translation**: Helsinki-NLP models (11+ languages)
- **Smart Q&A System**: Mistral 7B with FAISS vector search
- **Document Understanding**: Semantic chunking with overlap

### âš¡ **Performance Optimized**
- **Lazy-loaded models** with LRU caching
- **Batch processing** for translations
- **Memory management** with CUDA optimization
- **Persistent document storage** in memory

---

## ğŸ“Š Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PDF Upload    â”‚â”€â”€â”€â–¶â”‚  Text Extraction â”‚â”€â”€â”€â–¶â”‚   AI Processing â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Parallel Processing                  â–¼                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  Summarization  â”‚  â”‚  FAISS Indexing â”‚  â”‚  Text Cleaning  â”‚          â”‚
â”‚  â”‚   (BART-CNN)    â”‚  â”‚ (Embeddings)    â”‚  â”‚  (Preprocessing)â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Installation & Setup

### 1ï¸âƒ£ **Environment Setup**
```powershell
# Clone the repository
git clone https://github.com/A-ryanVAT-S/AI-Research-Copilot-Backend.git
cd AI-Research-Copilot-Backend

# Create virtual environment
python -m venv .venv
.venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2ï¸âƒ£ **PyTorch Configuration**
```powershell
# For CUDA 12.1+ (GPU acceleration)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# For CPU-only deployment
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
```

### 3ï¸âƒ£ **Download Required Models**
```powershell
# Download Mistral 7B GGUF model for Q&A
Invoke-WebRequest -Uri "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf" -OutFile "mistral-7b-instruct-v0.1.Q4_K_M.gguf"
```

### 4ï¸âƒ£ **Start the Server**
```powershell
# Development mode with auto-reload
uvicorn main:app --reload --host 0.0.0.0 --port 8000

# Production mode
uvicorn main:app --host 0.0.0.0 --port 8000
```

---

## ğŸ“š API Endpoints

### **File Management**
- `POST /api/upload` - Upload PDF and process immediately
- `GET /api/file-info/{doc_id}` - Get document metadata

### **AI Features**
- `GET /api/analyze/{doc_id}` - Get document summary
- `POST /api/translate/{doc_id}` - Translate summary to target language
- `POST /api/qa/{doc_id}` - Ask questions about the document

### **Example Request**
```python
import requests

# Upload and process document
files = {'file': open('research_paper.pdf', 'rb')}
response = requests.post('http://localhost:8000/api/upload', files=files)
doc_id = response.json()['doc_id']

# Get instant summary
summary = requests.get(f'http://localhost:8000/api/analyze/{doc_id}')

# Ask questions
qa_response = requests.post(f'http://localhost:8000/api/qa/{doc_id}', 
                           json={'question': 'What is the main contribution?'})
```

---

## ğŸ—ï¸ Project Structure

```
AI-Research-Copilot-Backend/
â”œâ”€â”€ main.py                 # FastAPI application & endpoints
â”œâ”€â”€ summaryModel.py         # BART-based summarization
â”œâ”€â”€ translationModel.py     # Helsinki-NLP translation models
â”œâ”€â”€ qaModel.py             # Mistral 7B Q&A with FAISS
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ files/                 # Uploaded documents storage
â””â”€â”€ __pycache__/          # Python bytecode cache
```

---

## ğŸ”§ Configuration

### **Environment Variables**
```bash
# Optional configuration
TORCH_DEVICE=cuda          # Force device selection
MODEL_CACHE_DIR=./models   # Custom model cache directory
MAX_FILE_SIZE=10485760     # File size limit (10MB)
```

### **Hardware Requirements**
- **Minimum**: 8GB RAM, 4-core CPU
- **Recommended**: 16GB RAM, 8-core CPU, NVIDIA GPU
- **Storage**: 10GB for models and cache
- **Python**: 3.9+ (tested with 3.12)

---

## ğŸš€ Performance Features

### **Memory Optimization**
- LRU cache for models (max 5 translation models)
- Automatic CUDA memory cleanup
- Batch processing for large documents

### **Processing Speed**
- **Upload to Summary**: ~30-60 seconds
- **Q&A Response**: ~5-10 seconds
- **Translation**: ~3-8 seconds

### **Scalability**
- Stateless design for horizontal scaling
- In-memory document store (can be replaced with Redis/DB)
- Lazy model loading reduces startup time

---

## ğŸ›¡ï¸ Security & Limitations

### **Security Features**
- File type validation (PDF/text only)
- File size limits (10MB max)
- Input sanitization and error handling

### **Current Limitations**
- **Memory Storage**: Documents stored in RAM (not persistent)
- **Single Instance**: No multi-user session management
- **Model Size**: Large models require significant RAM
- **Language Support**: 11 languages for translation

---

## ğŸ”„ Future Enhancements
- [ ] Database integration for persistent storage
- [ ] Redis caching for production deployment
- [ ] User authentication and session management
- [ ] Batch processing for multiple documents
- [ ] Additional model options and fine-tuning
- [ ] Real-time WebSocket updates for processing status

---

## ğŸ“ Support & Contributing

**Issues**: Report bugs and feature requests on GitHub  
**Documentation**: See `/docs` for detailed API documentation  
**Contributing**: Fork, branch, and submit pull requests  

**Tech Stack**: FastAPI, PyTorch, Transformers, LangChain, FAISS

